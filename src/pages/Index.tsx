
import React from 'react';
import ChatInterface from '@/components/ChatInterface';
import Banner from '@/components/Banner';
import CodeBlock from '@/components/CodeBlock';
import { GitHubLogoIcon } from '@radix-ui/react-icons';

const Index = () => {
  const installCode = `npm install openai`;
  
  const setupCode = `import OpenAI from 'openai';

// API-Endpoint des OVHcloud AI Endpoints
const API_ENDPOINT = 'https://deepseek-r1-distill-llama-70b.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v';

// API-Schl√ºssel (in einer echten Anwendung aus Umgebungsvariablen)
const API_KEY = 'Ihr-API-Schl√ºssel';

// OpenAI-Client initialisieren
const openai = new OpenAI({
  apiKey: API_KEY,
  baseURL: API_ENDPOINT,
  // Warnung: Nur f√ºr Demo-Zwecke!
  dangerouslyAllowBrowser: true 
});`;

  const usageCode = `// Nachricht an das Modell senden
async function chatWithAI(userMessage) {
  try {
    const response = await openai.chat.completions.create({
      model: 'deepseek-r1-distill-llama-70b',
      messages: [
        { role: 'system', content: 'Du bist ein hilfreicher Assistent.' },
        { role: 'user', content: userMessage }
      ],
      temperature: 0.7,
      max_tokens: 1000,
    });

    return response.choices[0].message.content;
  } catch (error) {
    console.error('Fehler bei der API-Anfrage:', error);
    throw error;
  }
}

// Beispielaufruf
chatWithAI('Erkl√§re die Vorteile von KI in einfachen Worten.')
  .then(response => console.log(response))
  .catch(error => console.error(error));`;

  const reactComponentCode = `import React, { useState } from 'react';
import OpenAI from 'openai';

// OpenAI-Client (normalerweise in einem separaten Service)
const openai = new OpenAI({
  apiKey: process.env.REACT_APP_OVH_API_KEY,
  baseURL: 'https://deepseek-r1-distill-llama-70b.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v',
  dangerouslyAllowBrowser: true
});

const AIChat = () => {
  const [prompt, setPrompt] = useState('');
  const [response, setResponse] = useState('');
  const [isLoading, setIsLoading] = useState(false);

  const handleSubmit = async (e) => {
    e.preventDefault();
    if (!prompt.trim()) return;

    setIsLoading(true);
    try {
      const result = await openai.chat.completions.create({
        model: 'deepseek-r1-distill-llama-70b',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.7,
      });

      setResponse(result.choices[0].message.content);
    } catch (error) {
      console.error('Fehler:', error);
      setResponse('Es ist ein Fehler aufgetreten. Bitte versuchen Sie es erneut.');
    } finally {
      setIsLoading(false);
    }
  };

  return (
    <div className="p-4">
      <form onSubmit={handleSubmit}>
        <input
          type="text"
          value={prompt}
          onChange={(e) => setPrompt(e.target.value)}
          placeholder="Frage stellen..."
          className="w-full p-2 mb-4 border rounded"
        />
        <button 
          type="submit" 
          disabled={isLoading}
          className="px-4 py-2 bg-blue-500 text-white rounded"
        >
          {isLoading ? 'Wird bearbeitet...' : 'Senden'}
        </button>
      </form>
      
      {response && (
        <div className="mt-4 p-4 bg-gray-100 rounded">
          <h3 className="font-bold mb-2">Antwort:</h3>
          <p>{response}</p>
        </div>
      )}
    </div>
  );
};

export default AIChat;`;

  return (
    <div className="min-h-screen w-full flex flex-col">
      <Banner />
      
      <main className="flex-1 container mx-auto px-4 py-12 md:py-16">
        <section className="mb-16 max-w-4xl mx-auto text-center animate-fade-up">
          <div className="inline-block px-3 py-1 rounded-full bg-white/5 mb-6">
            <span className="text-highlight text-sm font-medium">Jetzt verf√ºgbar</span>
          </div>
          <p className="text-lg md:text-xl text-white/80 mb-6 leading-relaxed">
            Mit den OVHcloud AI Endpoints bietet OVHcloud eine leistungsstarke M√∂glichkeit, k√ºnstliche Intelligenz einfach und skalierbar zu nutzen, ohne dass Nutzer sich um komplexe Infrastruktur k√ºmmern m√ºssen. Diese API-basierten L√∂sungen erm√∂glichen es, gro√üe Sprachmodelle (LLMs) und andere KI-Dienste direkt in eigene Anwendungen zu integrieren ‚Äì schnell, sicher und effizient.
          </p>
        </section>

        <section className="mb-16 max-w-4xl mx-auto animate-fade-up" style={{ animationDelay: "0.1s" }}>
          <div className="glass-morphism rounded-xl p-6 md:p-8">
            <h2 className="text-2xl md:text-3xl font-semibold mb-4">
              Neu verf√ºgbar: <span className="highlight-text">deepseek-r1-distill-llama-70b</span>
            </h2>
            <p className="text-white/80 mb-8 leading-relaxed">
              Seit Kurzem ist nun auch das leistungsstarke deepseek-r1-distill-llama-70b √ºber OVHcloud verf√ºgbar!
              Dieses Modell ist ein sogenanntes Reasoning-LLM ‚Äì spezialisiert auf logisches Denken, Probleml√∂sung und pr√§zise Antworten. Solche Modelle werden h√§ufig in Code-Generierung, komplexen Analysen und Entscheidungsunterst√ºtzung eingesetzt.
            </p>
          </div>
        </section>

        <section className="mb-16 max-w-4xl mx-auto text-center animate-fade-up" style={{ animationDelay: "0.2s" }}>
          <h2 className="text-2xl md:text-3xl font-semibold mb-4">
            Teste Deepseek-R1 jetzt selbst!
          </h2>
          <p className="text-white/80 mb-8">
            Anstatt nur dar√ºber zu lesen, kannst du dir selbst ein Bild machen!
            Unten findest du einen interaktiven Chatbot, der mit Deepseek l√§uft. Stelle ihm Fragen, teste seine F√§higkeiten und entdecke, wie leistungsstark moderne KI heute ist.
          </p>
          <div className="inline-block mb-8 highlight-text">
            ‚û° Probiere es aus! üñ•Ô∏èüí°
          </div>
        </section>
        
        <section className="animate-fade-up mb-16" style={{ animationDelay: "0.3s" }}>
          <ChatInterface />
        </section>

        {/* Tutorial Content moved from Tutorial.tsx */}
        <section className="mb-16 max-w-4xl mx-auto animate-fade-up" style={{ animationDelay: "0.4s" }}>
          <div className="glass-morphism rounded-xl p-6 md:p-8 mb-12">
            <h2 className="text-2xl md:text-3xl font-semibold mb-4">
              Integration von OVHcloud AI Endpoints
            </h2>
            <p className="text-white/80 mb-8 leading-relaxed text-lg">
              In diesem Tutorial erfahren Sie, wie Sie die OVHcloud AI Endpoints in Ihre Anwendungen integrieren k√∂nnen. 
              Die folgende Anleitung zeigt Ihnen Schritt f√ºr Schritt, wie Sie das aktuelle Deepseek-R1 Large Language Model (LLM) 
              √ºber die OVHcloud AI Endpoints API anbinden und nutzen k√∂nnen. Mit den bereitgestellten Code-Beispielen 
              k√∂nnen Sie schnell und einfach eigene KI-gest√ºtzte Anwendungen entwickeln.
            </p>
            
            <div className="bg-black/20 rounded-lg p-4 border border-white/10">
              <div className="flex items-center mb-2">
                <GitHubLogoIcon className="w-5 h-5 mr-2" />
                <h3 className="text-lg font-medium">Open Source Initiative</h3>
              </div>
              <p className="text-white/80 text-sm">
                Im Sinne der OVHcloud Open-Source-Philosophie ist dieses Tutorial und der gesamte Quellcode √∂ffentlich verf√ºgbar. 
                Synaigy legt den vollst√§ndigen Code offen, um Transparenz zu f√∂rdern und der Community die M√∂glichkeit zu geben, 
                daran mitzuwirken und ihn weiterzuentwickeln.
              </p>
              <div className="mt-3">
                <a 
                  href="https://github.com/Synaigy/ovh-chatbot" 
                  target="_blank" 
                  rel="noopener noreferrer"
                  className="inline-flex items-center px-3 py-1 text-sm bg-white/10 hover:bg-white/20 rounded-full transition-colors"
                >
                  <GitHubLogoIcon className="w-4 h-4 mr-1" />
                  GitHub Repository ansehen
                </a>
              </div>
            </div>
          </div>
          
          <div className="glass-morphism rounded-xl p-6 md:p-8 mb-12">
            <h2 className="text-2xl font-semibold mb-4">Erste Schritte mit OVHcloud AI Endpoints</h2>
            <p className="text-white/80 mb-6 leading-relaxed">
              Die Integration von OVHcloud AI Endpoints in Ihre Anwendung ist unkompliziert. Mit der OpenAI-kompatiblen API k√∂nnen Sie die DeepSeek-Modelle in Ihre Anwendungen einbinden, ohne Ihre bestehende OpenAI-basierte Implementierung grundlegend √§ndern zu m√ºssen.
            </p>
            
            <div className="space-y-8">
              <div>
                <h3 className="text-xl font-medium mb-2 highlight-text">1. Installation der OpenAI-Bibliothek</h3>
                <p className="text-white/80 mb-4">
                  Da die OVHcloud AI Endpoints mit der OpenAI-API kompatibel sind, k√∂nnen wir die offizielle OpenAI-Bibliothek verwenden:
                </p>
                <CodeBlock language="bash" code={installCode} />
              </div>
              
              <div>
                <h3 className="text-xl font-medium mb-2 highlight-text">2. Einrichtung des Clients</h3>
                <p className="text-white/80 mb-4">
                  Konfigurieren Sie den OpenAI-Client mit dem OVHcloud API-Endpunkt:
                </p>
                <CodeBlock language="javascript" code={setupCode} />
              </div>
              
              <div>
                <h3 className="text-xl font-medium mb-2 highlight-text">3. Verwendung des Modells</h3>
                <p className="text-white/80 mb-4">
                  Jetzt k√∂nnen Sie Anfragen an das DeepSeek-Modell senden:
                </p>
                <CodeBlock language="javascript" code={usageCode} />
              </div>
              
              <div>
                <h3 className="text-xl font-medium mb-2 highlight-text">4. Integration in eine React-Anwendung</h3>
                <p className="text-white/80 mb-4">
                  Hier ein Beispiel f√ºr eine einfache React-Komponente, die mit dem Modell interagiert:
                </p>
                <CodeBlock language="jsx" code={reactComponentCode} />
              </div>
            </div>
          </div>
          
          <div className="glass-morphism rounded-xl p-6 md:p-8">
            <h2 className="text-2xl font-semibold mb-4">Wichtige Hinweise</h2>
            <ul className="space-y-4 text-white/80">
              <li className="flex items-start">
                <span className="text-highlight mr-2">‚Ä¢</span>
                <span>Verwenden Sie in produktiven Anwendungen <strong>niemals</strong> API-Schl√ºssel direkt im Frontend-Code. Nutzen Sie stattdessen einen Backend-Service oder Umgebungsvariablen.</span>
              </li>
              <li className="flex items-start">
                <span className="text-highlight mr-2">‚Ä¢</span>
                <span>Die Option <code>dangerouslyAllowBrowser: true</code> sollte nur f√ºr Demonstrations- und Entwicklungszwecke verwendet werden.</span>
              </li>
              <li className="flex items-start">
                <span className="text-highlight mr-2">‚Ä¢</span>
                <span>Implementieren Sie geeignete Fehlerbehandlung und Rate-Limiting in Ihren Anwendungen.</span>
              </li>
              <li className="flex items-start">
                <span className="text-highlight mr-2">‚Ä¢</span>
                <span>F√ºr umfangreiche Anwendungen empfiehlt sich die Implementierung von Caching-Strategien, um API-Aufrufe zu minimieren.</span>
              </li>
            </ul>
          </div>
        </section>
      </main>
    </div>
  );
};

export default Index;
